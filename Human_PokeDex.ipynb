{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human PokeDex.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/newb-dev-1008/Human-PokeDex/blob/colab/Human_PokeDex.ipynb",
      "authorship_tag": "ABX9TyMHbgz6W8BYOugjMdxOho2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVTet-sDU9kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0d8d22-ccf7-46d0-cf4d-e2001aad4a00"
      },
      "source": [
        "!pip install --upgrade imutils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdo-ICq615RJ",
        "outputId": "19382377-4e72-461e-fb60-4905b7a3df23"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjn5BlIo1ekW"
      },
      "source": [
        "# **Extract embeddings from face dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6NxT6A0FZ8"
      },
      "source": [
        "# Importing all necessary packages\r\n",
        "\r\n",
        "from imutils import paths\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P3w76101_7J",
        "outputId": "230432e6-9ccb-4035-b7ad-0a360c3ab600"
      },
      "source": [
        "# Load face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\")\r\n",
        "\r\n",
        "protoPath = os.path.sep.join(['/content', 'deploy.prototxt'])\r\n",
        "modelPath = os.path.sep.join(['/content', 'res10_300x300_ssd_iter_140000.caffemodel'])\r\n",
        "\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "\r\n",
        "\r\n",
        "# Load face recognizer\r\n",
        "\r\n",
        "print(\"\\nLoading face recognizer...\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/openface_nn4.small2.v1.t7')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face detector...\n",
            "\n",
            "Loading face recognizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDk_iHf1OO2Q",
        "outputId": "9679f321-0c9d-41e0-8155-703f42bac35f"
      },
      "source": [
        "# Entering paths to our images dataset\r\n",
        "\r\n",
        "print('\\nQuantifying faces...')\r\n",
        "imagePaths = list(paths.list_images('/content/drive/MyDrive/Open Lab/Datasets'))\r\n",
        "\r\n",
        "knownEmbeddings = []\r\n",
        "knownNames = []\r\n",
        "\r\n",
        "# Total number of faces\r\n",
        "total = 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Quantifying faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036MFBrCQFz6",
        "outputId": "1f71ef28-7201-4da3-90f1-fdb03162c911"
      },
      "source": [
        "# Loop over all image paths\r\n",
        "\r\n",
        "for (i, imagePath) in enumerate(imagePaths):\r\n",
        "  print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))           # Extract the person name from the image path\r\n",
        "  name = imagePath.split(os.path.sep)[-2]\r\n",
        "\r\n",
        "  image = cv2.imread(imagePath)                                                   # Load the image\r\n",
        "  image = imutils.resize(image, width=600)                                        # Resize to (600, 600)\r\n",
        "  (h, w) = image.shape[:2]                                                        # Store image dimensions\r\n",
        "  imageBlob = cv2.dnn.blobFromImage(                                              # Construct a blob from the image\r\n",
        "  \tcv2.resize(image, (300, 300)), 1.0, (300, 300),\r\n",
        "  \t(104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n",
        "\r\n",
        "  detector.setInput(imageBlob)                                                    # Face Detector to localize face in an image\r\n",
        "  detections = detector.forward()       \r\n",
        "\r\n",
        "  if len(detections) > 0:                                                         # Ensure at least one face was found\r\n",
        "    i = np.argmax(detections[0, 0, :, 2])\r\n",
        "    confidence = detections[0, 0, i, 2]        \r\n",
        "\r\n",
        "    if (confidence > 0.1):                                                          # Filtering weak detections\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\t\t\t\r\n",
        "      face = image[startY:endY, startX:endX]\r\n",
        "      (fH, fW) = face.shape[:2]\r\n",
        "\t\t\t\r\n",
        "      if (fW < 20 or fH < 20):\r\n",
        "        continue    \r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), \r\n",
        "                                       swapRB=True, crop=False)                   # Create blob\r\n",
        "\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      knownNames.append(name)                                                     # Append name to list\r\n",
        "      knownEmbeddings.append(vec.flatten())                                       # Append flattened embedding to list\r\n",
        "      total += 1                                                "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing image 1/25\n",
            "[INFO] processing image 2/25\n",
            "[INFO] processing image 3/25\n",
            "[INFO] processing image 4/25\n",
            "[INFO] processing image 5/25\n",
            "[INFO] processing image 6/25\n",
            "[INFO] processing image 7/25\n",
            "[INFO] processing image 8/25\n",
            "[INFO] processing image 9/25\n",
            "[INFO] processing image 10/25\n",
            "[INFO] processing image 11/25\n",
            "[INFO] processing image 12/25\n",
            "[INFO] processing image 13/25\n",
            "[INFO] processing image 14/25\n",
            "[INFO] processing image 15/25\n",
            "[INFO] processing image 16/25\n",
            "[INFO] processing image 17/25\n",
            "[INFO] processing image 18/25\n",
            "[INFO] processing image 19/25\n",
            "[INFO] processing image 20/25\n",
            "[INFO] processing image 21/25\n",
            "[INFO] processing image 22/25\n",
            "[INFO] processing image 23/25\n",
            "[INFO] processing image 24/25\n",
            "[INFO] processing image 25/25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glLTq5YeYOLZ",
        "outputId": "c571f3a4-b7af-4d45-b198-cb2f4739339b"
      },
      "source": [
        "# Save (pickle) the embeddings and the names\r\n",
        "\r\n",
        "print ('Serializing {} encodings:\\n'.format(total))\r\n",
        "\r\n",
        "data = {'embeddings': knownEmbeddings, 'names': knownNames}                       # Saved as a dictionary/ HashMap\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'wb')\r\n",
        "f.write(pickle.dumps(data))                                                       # Stored as a ByteStream\r\n",
        "f.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serializing 25 encodings:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dlH_sXrZmx3"
      },
      "source": [
        "# **Train the face recognition model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EeL68fqZmUy"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.svm import SVC\r\n",
        "import argparse\r\n",
        "import pickle"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "tQXtEKrgdXGy",
        "outputId": "adb09246-897f-4de8-9248-099b46bd7cb3"
      },
      "source": [
        "print(\"Loading face embeddings:\\n\")\r\n",
        "data = pickle.loads(open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'rb').read())\r\n",
        "print(\"Loaded.\\n\\n\")\r\n",
        "\r\n",
        "print(\"Encoding labels:\\n\")\r\n",
        "le = LabelEncoder()\r\n",
        "labels = le.fit_transform(data['names'])\r\n",
        "print(\"\\nLabels encoded.\\n\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face embeddings:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f9dd8036a4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading face embeddings:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded.\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoding labels:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not '_io.BufferedReader'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QoQw-Auiolr"
      },
      "source": [
        "print(\"Training model...\\n\")\r\n",
        "recognizer = SVC(C = 1.0, kernel = \"linear\", probability = True)\r\n",
        "recognizer.fit(data[\"embeddings\"], labels)\r\n",
        "print(\"\\nModel trained.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RnEopljrS2"
      },
      "source": [
        "# Save the actual face recognition model\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(recognizer))\r\n",
        "f.close()\r\n",
        "\r\n",
        "# Save the label encoder\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(le))\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}