{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human PokeDex.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb",
      "authorship_tag": "ABX9TyPWhBVJ43A2NOhFfxqLUJTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVTet-sDU9kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1afa34-e14c-4dc0-dd51-aa49ff888f11"
      },
      "source": [
        "!pip install --upgrade imutils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdo-ICq615RJ",
        "outputId": "367548b0-a0c6-48f5-ae29-f7023484388f"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjn5BlIo1ekW"
      },
      "source": [
        "# **Extract embeddings from face dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6NxT6A0FZ8"
      },
      "source": [
        "# Importing all necessary packages\r\n",
        "\r\n",
        "from imutils import paths\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P3w76101_7J",
        "outputId": "4eef0f4c-7575-4863-a42c-2d8c6134e12f"
      },
      "source": [
        "# Load face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\")\r\n",
        "\r\n",
        "protoPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'deploy.prototxt'])\r\n",
        "modelPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'res10_300x300_ssd_iter_140000.caffemodel'])\r\n",
        "\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "\r\n",
        "\r\n",
        "# Load face recognizer\r\n",
        "\r\n",
        "print(\"\\nLoading face recognizer...\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/drive/MyDrive/Open Lab/openface_nn4.small2.v1.t7')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face detector...\n",
            "\n",
            "Loading face recognizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDk_iHf1OO2Q",
        "outputId": "0de51587-b349-49ca-bd33-ff1f9ee1c9e1"
      },
      "source": [
        "# Entering paths to our images dataset\r\n",
        "\r\n",
        "print('\\nQuantifying faces...')\r\n",
        "imagePaths = list(paths.list_images('/content/drive/MyDrive/Open Lab/Datasets'))\r\n",
        "\r\n",
        "knownEmbeddings = []\r\n",
        "knownNames = []\r\n",
        "\r\n",
        "# Total number of faces\r\n",
        "total = 0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Quantifying faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036MFBrCQFz6"
      },
      "source": [
        "# Loop over all image paths\r\n",
        "\r\n",
        "for (i, imagePath) in enumerate(imagePaths):\r\n",
        "  print(\"Processing image {}/{}\".format(i + 1, len(imagePaths)))                  # Extract the person name from the image path\r\n",
        "  name = imagePath.split(os.path.sep)[-2]\r\n",
        "\r\n",
        "  image = cv2.imread(imagePath)                                                   # Load the image\r\n",
        "  image = imutils.resize(image, width=600)                                        # Resize to (600, 600)\r\n",
        "  (h, w) = image.shape[:2]                                                        # Store image dimensions\r\n",
        "  imageBlob = cv2.dnn.blobFromImage(                                              # Construct a blob from the image\r\n",
        "  \tcv2.resize(image, (300, 300)), 1.0, (300, 300),\r\n",
        "  \t(104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n",
        "\r\n",
        "  detector.setInput(imageBlob)                                                    # Face Detector to localize face in an image\r\n",
        "  detections = detector.forward()       \r\n",
        "\r\n",
        "  if len(detections) > 0:                                                         # Ensure at least one face was found\r\n",
        "    i = np.argmax(detections[0, 0, :, 2])\r\n",
        "    confidence = detections[0, 0, i, 2]        \r\n",
        "\r\n",
        "    if (confidence > 0.1):                                                          # Filtering weak detections\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\t\t\t\r\n",
        "      face = image[startY:endY, startX:endX]\r\n",
        "      (fH, fW) = face.shape[:2]\r\n",
        "\t\t\t\r\n",
        "      if (fW < 20 or fH < 20):\r\n",
        "        continue    \r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), \r\n",
        "                                       swapRB=True, crop=False)                   # Create blob\r\n",
        "\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      knownNames.append(name)                                                     # Append name to list\r\n",
        "      knownEmbeddings.append(vec.flatten())                                       # Append flattened embedding to list\r\n",
        "      total += 1                                                "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glLTq5YeYOLZ",
        "outputId": "327c3dd1-b27b-461d-ddf4-2e4873c07cc3"
      },
      "source": [
        "# Save (pickle) the embeddings and the names\r\n",
        "\r\n",
        "print ('Serializing {} encodings:\\n'.format(total))\r\n",
        "\r\n",
        "data = {'embeddings': knownEmbeddings, 'names': knownNames}                       # Saved as a dictionary/ HashMap\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'wb')\r\n",
        "f.write(pickle.dumps(data))                                                       # Stored as a ByteStream\r\n",
        "f.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serializing 0 encodings:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dlH_sXrZmx3"
      },
      "source": [
        "# **Train the face recognition model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EeL68fqZmUy"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.svm import SVC\r\n",
        "import argparse\r\n",
        "import pickle"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQXtEKrgdXGy",
        "outputId": "aa4decc9-c2c3-4286-b551-9f8be6894c7e"
      },
      "source": [
        "print(\"Loading face embeddings:\\n\")\r\n",
        "data = pickle.loads(open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'rb').read())\r\n",
        "print(\"Loaded.\\n\\n\")\r\n",
        "\r\n",
        "print(\"Encoding labels:\\n\")\r\n",
        "le = LabelEncoder()\r\n",
        "labels = le.fit_transform(data['names'])\r\n",
        "print(\"\\nLabels encoded.\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face embeddings:\n",
            "\n",
            "Loaded.\n",
            "\n",
            "\n",
            "Encoding labels:\n",
            "\n",
            "\n",
            "Labels encoded.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "1QoQw-Auiolr",
        "outputId": "3babfa41-552d-49da-eda5-0d2be978708e"
      },
      "source": [
        "print(\"Training model...\\n\")\r\n",
        "recognizer = SVC(C = 1.0, kernel = \"linear\", probability = True)\r\n",
        "recognizer.fit(data[\"embeddings\"], labels)\r\n",
        "print(\"\\nModel trained.\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-03b55838c8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nModel trained.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    146\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    147\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RnEopljrS2"
      },
      "source": [
        "# Save the actual face recognition model\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(recognizer))\r\n",
        "f.close()\r\n",
        "\r\n",
        "# Save the label encoder\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(le))\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmQiypWqk1MV"
      },
      "source": [
        "# **Recognise faces using OpenCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDVTA-esk6Wo"
      },
      "source": [
        "from imutils.video import VideoStream\r\n",
        "from imutils.video import FPS\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXSzYgIyk-za"
      },
      "source": [
        "# Load our serialized face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\\n\")\r\n",
        "protoPath = os.path.sep.join(['/content', \"deploy.prototxt\"])\r\n",
        "modelPath = os.path.sep.join(['/content', \r\n",
        "                              \"res10_300x300_ssd_iter_140000.caffemodel\"])\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "print(\"Loaded face detector.\\n\")\r\n",
        "\r\n",
        "# Load our serialized face embedding model\r\n",
        "print(\"\\nLoading face recognizer...\\n\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/openface_nn4.small2.v1.t7')\r\n",
        "print(\"Loaded Face Recognizer.\\n\")\r\n",
        "\r\n",
        "# Load the SVM Model and LabelEncoder\r\n",
        "recognizer = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"rb\").read())\r\n",
        "le = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"rb\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ope1aUvrlJth"
      },
      "source": [
        "# Initialize the video stream, then allow the camera sensor to warm up\r\n",
        "\r\n",
        "print(\"Starting video stream...\\n\")\r\n",
        "\r\n",
        "vs = VideoStream(src=0).start()\r\n",
        "time.sleep(2.0)\r\n",
        "\r\n",
        "# Start FPS Throughput Estimator\r\n",
        "fps = FPS().start()\r\n",
        "\r\n",
        "# Loop over frames\r\n",
        "while True:\r\n",
        "\tframe = vs.read()\r\n",
        "\tframe = imutils.resize(frame, width=600)                                        # Resize frames to (600, 600) and maintain ratio\r\n",
        "\t(h, w) = frame.shape[:2]\r\n",
        "\r\n",
        "\timageBlob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, \r\n",
        "                                   (300, 300), (104.0, 177.0, 123.0), \r\n",
        "                                   swapRB=False, crop=False)                      # Construct a blob from the image\r\n",
        "\t\r\n",
        "\tdetector.setInput(imageBlob)                                                    # Detect face and localize within image\r\n",
        "\tdetections = detector.forward()\r\n",
        " \r\n",
        "  # Loop over the detections\r\n",
        "\r\n",
        "  for i in range(0, detections.shape[2]):\r\n",
        "    confidence = detections[0, 0, i, 2]\r\n",
        "\r\n",
        "    if (confidence > 0.2):\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])                     # Coordinates of box enclosing face\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\r\n",
        "      face = frame[startY:endY, startX:endX]\r\n",
        "      (fH, fW) = face.shape[:2]\r\n",
        "\r\n",
        "      if fW < 20 or fH < 20:\r\n",
        "        continue\r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), \r\n",
        "                                         (0, 0, 0), swapRB=True, crop=False)\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      preds = recognizer.predict_proba(vec)[0]                                    # Classification to recognise a face\r\n",
        "      j = np.argmax(preds)\r\n",
        "      proba = preds[j]\r\n",
        "      name = le.classes_[j]\r\n",
        "\r\n",
        "      text = \"{}: {:.2f}%\".format(name, proba * 100)                              # Bounding box with name and probability\r\n",
        "      y = startY - 10 if startY - 10 > 10 else startY + 10\r\n",
        "\r\n",
        "      cv2.rectangle(frame, (startX, startY), (endX, endY), \r\n",
        "                      (0, 0, 255), 2)\r\n",
        "      cv2.putText(frame, text, (startX, y), \r\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\r\n",
        "\t# update the FPS counter\r\n",
        "\tfps.update()\r\n",
        " \r\n",
        "  cv2.imshow(\"Frame\", frame)\r\n",
        "\r\n",
        "\tkey = cv2.waitKey(1) & 0xFF\r\n",
        "\r\n",
        "\tif (key == ord(\"q\") or key == ord(\"Q\")):\r\n",
        "\t\tbreak                                                                         # Press 'Q' to break out of the loop\r\n",
        "\r\n",
        "# Stop the timer and display FPS information\r\n",
        "fps.stop()\r\n",
        "\r\n",
        "print(\"Elasped time: {:.2f}\".format(fps.elapsed()))\r\n",
        "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\r\n",
        "\r\n",
        "cv2.destroyAllWindows()\r\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBWF79rjrRoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}