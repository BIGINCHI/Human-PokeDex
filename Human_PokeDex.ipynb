{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human PokeDex.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/newb-dev-1008/Human-PokeDex/blob/colab/Human_PokeDex.ipynb",
      "authorship_tag": "ABX9TyPERxQFtgX4UbbQg/3TL/w6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVTet-sDU9kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0d8d22-ccf7-46d0-cf4d-e2001aad4a00"
      },
      "source": [
        "!pip install --upgrade imutils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdo-ICq615RJ",
        "outputId": "19382377-4e72-461e-fb60-4905b7a3df23"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjn5BlIo1ekW"
      },
      "source": [
        "# **Extract embeddings from face dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6NxT6A0FZ8"
      },
      "source": [
        "# Importing all necessary packages\r\n",
        "\r\n",
        "from imutils import paths\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P3w76101_7J",
        "outputId": "230432e6-9ccb-4035-b7ad-0a360c3ab600"
      },
      "source": [
        "# Load face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\")\r\n",
        "\r\n",
        "protoPath = os.path.sep.join(['/content', 'deploy.prototxt'])\r\n",
        "modelPath = os.path.sep.join(['/content', 'res10_300x300_ssd_iter_140000.caffemodel'])\r\n",
        "\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "\r\n",
        "\r\n",
        "# Load face recognizer\r\n",
        "\r\n",
        "print(\"\\nLoading face recognizer...\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/openface_nn4.small2.v1.t7')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face detector...\n",
            "\n",
            "Loading face recognizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDk_iHf1OO2Q",
        "outputId": "9679f321-0c9d-41e0-8155-703f42bac35f"
      },
      "source": [
        "# Entering paths to our images dataset\r\n",
        "\r\n",
        "print('\\nQuantifying faces...')\r\n",
        "imagePaths = list(paths.list_images('/content/drive/MyDrive/Open Lab/Datasets'))\r\n",
        "\r\n",
        "knownEmbeddings = []\r\n",
        "knownNames = []\r\n",
        "\r\n",
        "# Total number of faces\r\n",
        "total = 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Quantifying faces...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036MFBrCQFz6",
        "outputId": "1f71ef28-7201-4da3-90f1-fdb03162c911"
      },
      "source": [
        "# Loop over all image paths\r\n",
        "\r\n",
        "for (i, imagePath) in enumerate(imagePaths):\r\n",
        "  print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))           # Extract the person name from the image path\r\n",
        "  name = imagePath.split(os.path.sep)[-2]\r\n",
        "\r\n",
        "  image = cv2.imread(imagePath)                                                   # Load the image\r\n",
        "  image = imutils.resize(image, width=600)                                        # Resize to (600, 600)\r\n",
        "  (h, w) = image.shape[:2]                                                        # Store image dimensions\r\n",
        "  imageBlob = cv2.dnn.blobFromImage(                                              # Construct a blob from the image\r\n",
        "  \tcv2.resize(image, (300, 300)), 1.0, (300, 300),\r\n",
        "  \t(104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n",
        "\r\n",
        "  detector.setInput(imageBlob)                                                    # Face Detector to localize face in an image\r\n",
        "  detections = detector.forward()       \r\n",
        "\r\n",
        "  if len(detections) > 0:                                                         # Ensure at least one face was found\r\n",
        "    i = np.argmax(detections[0, 0, :, 2])\r\n",
        "    confidence = detections[0, 0, i, 2]        \r\n",
        "\r\n",
        "    if (confidence > 0.1):                                                          # Filtering weak detections\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\t\t\t\r\n",
        "      face = image[startY:endY, startX:endX]\r\n",
        "      (fH, fW) = face.shape[:2]\r\n",
        "\t\t\t\r\n",
        "      if (fW < 20 or fH < 20):\r\n",
        "        continue    \r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), \r\n",
        "                                       swapRB=True, crop=False)                   # Create blob\r\n",
        "\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      knownNames.append(name)                                                     # Append name to list\r\n",
        "      knownEmbeddings.append(vec.flatten())                                       # Append flattened embedding to list\r\n",
        "      total += 1                                                "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing image 1/25\n",
            "[INFO] processing image 2/25\n",
            "[INFO] processing image 3/25\n",
            "[INFO] processing image 4/25\n",
            "[INFO] processing image 5/25\n",
            "[INFO] processing image 6/25\n",
            "[INFO] processing image 7/25\n",
            "[INFO] processing image 8/25\n",
            "[INFO] processing image 9/25\n",
            "[INFO] processing image 10/25\n",
            "[INFO] processing image 11/25\n",
            "[INFO] processing image 12/25\n",
            "[INFO] processing image 13/25\n",
            "[INFO] processing image 14/25\n",
            "[INFO] processing image 15/25\n",
            "[INFO] processing image 16/25\n",
            "[INFO] processing image 17/25\n",
            "[INFO] processing image 18/25\n",
            "[INFO] processing image 19/25\n",
            "[INFO] processing image 20/25\n",
            "[INFO] processing image 21/25\n",
            "[INFO] processing image 22/25\n",
            "[INFO] processing image 23/25\n",
            "[INFO] processing image 24/25\n",
            "[INFO] processing image 25/25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glLTq5YeYOLZ",
        "outputId": "c571f3a4-b7af-4d45-b198-cb2f4739339b"
      },
      "source": [
        "# Save (pickle) the embeddings and the names\r\n",
        "\r\n",
        "print ('Serializing {} encodings:\\n'.format(total))\r\n",
        "\r\n",
        "data = {'embeddings': knownEmbeddings, 'names': knownNames}                       # Saved as a dictionary/ HashMap\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'wb')\r\n",
        "f.write(pickle.dumps(data))                                                       # Stored as a ByteStream\r\n",
        "f.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serializing 25 encodings:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dlH_sXrZmx3"
      },
      "source": [
        "# **Train the face recognition model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EeL68fqZmUy"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.svm import SVC\r\n",
        "import argparse\r\n",
        "import pickle"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQXtEKrgdXGy",
        "outputId": "7a27db6f-3721-41db-f63e-aaa3c406b07c"
      },
      "source": [
        "print(\"Loading face embeddings:\\n\")\r\n",
        "data = pickle.loads(open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'rb').read())\r\n",
        "print(\"Loaded.\\n\\n\")\r\n",
        "\r\n",
        "print(\"Encoding labels:\\n\")\r\n",
        "le = LabelEncoder()\r\n",
        "labels = le.fit_transform(data['names'])\r\n",
        "print(\"\\nLabels encoded.\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face embeddings:\n",
            "\n",
            "Loaded.\n",
            "\n",
            "\n",
            "Encoding labels:\n",
            "\n",
            "\n",
            "Labels encoded.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QoQw-Auiolr",
        "outputId": "77670504-34fa-49b2-e836-6356d7d0217f"
      },
      "source": [
        "print(\"Training model...\\n\")\r\n",
        "recognizer = SVC(C = 1.0, kernel = \"linear\", probability = True)\r\n",
        "recognizer.fit(data[\"embeddings\"], labels)\r\n",
        "print(\"\\nModel trained.\\n\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "\n",
            "\n",
            "Model trained.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RnEopljrS2"
      },
      "source": [
        "# Save the actual face recognition model\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(recognizer))\r\n",
        "f.close()\r\n",
        "\r\n",
        "# Save the label encoder\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"wb\")\r\n",
        "f.write(pickle.dumps(le))\r\n",
        "f.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmQiypWqk1MV"
      },
      "source": [
        "# **Recognise faces using OpenCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDVTA-esk6Wo"
      },
      "source": [
        "from imutils.video import VideoStream\r\n",
        "from imutils.video import FPS\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXSzYgIyk-za",
        "outputId": "0227dcbe-7271-4927-ac5b-2cfe910b2f55"
      },
      "source": [
        "# Load our serialized face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\\n\")\r\n",
        "protoPath = os.path.sep.join(['/content', \"deploy.prototxt\"])\r\n",
        "modelPath = os.path.sep.join(['/content', \r\n",
        "                              \"res10_300x300_ssd_iter_140000.caffemodel\"])\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "print(\"Loaded face detector.\\n\")\r\n",
        "\r\n",
        "# Load our serialized face embedding model\r\n",
        "print(\"\\nLoading face recognizer...\\n\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/openface_nn4.small2.v1.t7')\r\n",
        "print(\"Loaded Face Recognizer.\\n\")\r\n",
        "\r\n",
        "# Load the SVM Model and LabelEncoder\r\n",
        "recognizer = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"rb\").read())\r\n",
        "le = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"rb\").read())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading face detector...\n",
            "\n",
            "Loaded face detector.\n",
            "\n",
            "\n",
            "Loading face recognizer...\n",
            "\n",
            "Loaded Face Recognizer.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "ope1aUvrlJth",
        "outputId": "3c804625-459b-4fd2-d231-c6b173bc893b"
      },
      "source": [
        "# Initialize the video stream, then allow the camera sensor to warm up\r\n",
        "\r\n",
        "print(\"Starting video stream...\\n\")\r\n",
        "\r\n",
        "vs = VideoStream(src=0).start()\r\n",
        "time.sleep(2.0)\r\n",
        "\r\n",
        "# Start FPS Throughput Estimator\r\n",
        "fps = FPS().start()\r\n",
        "\r\n",
        "# Loop over frames\r\n",
        "while True:\r\n",
        "\tframe = vs.read()\r\n",
        "\tframe = imutils.resize(frame, width=600)                                        # Resize frames to (600, 600) and maintain ratio\r\n",
        "\t(h, w) = frame.shape[:2]\r\n",
        "\r\n",
        "\timageBlob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, \r\n",
        "                                   (300, 300), (104.0, 177.0, 123.0), \r\n",
        "                                   swapRB=False, crop=False)                      # Construct a blob from the image\r\n",
        "\t\r\n",
        "\tdetector.setInput(imageBlob)                                                    # Detect face and localize within image\r\n",
        "\tdetections = detector.forward()\r\n",
        " \r\n",
        "  # Loop over the detections\r\n",
        "\r\n",
        "  for i in range(0, detections.shape[2]):\r\n",
        "    confidence = detections[0, 0, i, 2]\r\n",
        "\r\n",
        "    if (confidence > 0.2):\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])                     # Coordinates of box enclosing face\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\r\n",
        "      face = frame[startY:endY, startX:endX]\r\n",
        "      (fH, fW) = face.shape[:2]\r\n",
        "\r\n",
        "      if fW < 20 or fH < 20:\r\n",
        "        continue\r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), \r\n",
        "                                         (0, 0, 0), swapRB=True, crop=False)\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      preds = recognizer.predict_proba(vec)[0]                                    # Classification to recognise a face\r\n",
        "      j = np.argmax(preds)\r\n",
        "      proba = preds[j]\r\n",
        "      name = le.classes_[j]\r\n",
        "\r\n",
        "      text = \"{}: {:.2f}%\".format(name, proba * 100)                              # Bounding box with name and probability\r\n",
        "      y = startY - 10 if startY - 10 > 10 else startY + 10\r\n",
        "\r\n",
        "      cv2.rectangle(frame, (startX, startY), (endX, endY), \r\n",
        "                      (0, 0, 255), 2)\r\n",
        "      cv2.putText(frame, text, (startX, y), \r\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\r\n",
        "\t# update the FPS counter\r\n",
        "\tfps.update()\r\n",
        " \r\n",
        "  cv2.imshow(\"Frame\", frame)\r\n",
        "\r\n",
        "\tkey = cv2.waitKey(1) & 0xFF\r\n",
        "\r\n",
        "\tif (key == ord(\"q\") or key == ord(\"Q\")):\r\n",
        "\t\tbreak                                                                         # Press 'Q' to break out of the loop\r\n",
        "\r\n",
        "# Stop the timer and display FPS information\r\n",
        "fps.stop()\r\n",
        "\r\n",
        "print(\"Elasped time: {:.2f}\".format(fps.elapsed()))\r\n",
        "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\r\n",
        "\r\n",
        "cv2.destroyAllWindows()\r\n",
        "vs.stop()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-f4767646a99c>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    for i in range(0, detections.shape[2]):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBWF79rjrRoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}