{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human PokeDex.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/newb-dev-1008/Human-PokeDex/blob/colab/Human_PokeDex.ipynb",
      "authorship_tag": "ABX9TyMOc9T5IK/xvc/Nuc7pMFn9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVTet-sDU9kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0d8d22-ccf7-46d0-cf4d-e2001aad4a00"
      },
      "source": [
        "!pip install --upgrade imutils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdo-ICq615RJ",
        "outputId": "19382377-4e72-461e-fb60-4905b7a3df23"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjn5BlIo1ekW"
      },
      "source": [
        "# **Extract embeddings from face dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6NxT6A0FZ8"
      },
      "source": [
        "# Importing all necessary packages\r\n",
        "\r\n",
        "from imutils import paths\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P3w76101_7J"
      },
      "source": [
        "# Load face detector\r\n",
        "\r\n",
        "print(\"Loading face detector...\")\r\n",
        "\r\n",
        "protoPath = os.path.sep.join(['/content', 'deploy.prototxt'])\r\n",
        "modelPath = os.path.sep.join(['/content', 'res10_300x300_ssd_iter_140000.caffemodel'])\r\n",
        "\r\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\r\n",
        "\r\n",
        "\r\n",
        "# Load face recognizer\r\n",
        "\r\n",
        "print(\"\\nLoading face recognizer...\")\r\n",
        "embedder = cv2.dnn.readNetFromTorch('/content/openface_nn4.small2.v1.t7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDk_iHf1OO2Q"
      },
      "source": [
        "# Entering paths to our images dataset\r\n",
        "\r\n",
        "print('\\nQuantifying faces...')\r\n",
        "imagePaths = list(paths.list_images('/content/drive/MyDrive/Open Lab/Datasets'))\r\n",
        "\r\n",
        "knownEmbeddings = []\r\n",
        "knownNames = []\r\n",
        "\r\n",
        "# Total number of faces\r\n",
        "total = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036MFBrCQFz6"
      },
      "source": [
        "# Loop over all image paths\r\n",
        "\r\n",
        "for (i, imagePath) in enumerate(imagePaths):\r\n",
        "  print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))           # Extract the person name from the image path\r\n",
        "  name = imagePath.split(os.path.sep)[-2]\r\n",
        "\r\n",
        "  image = cv2.imread(imagePath)                                                   # Load the image\r\n",
        "  image = imutils.resize(image, width=600)                                        # Resize to (600, 600)\r\n",
        "  (h, w) = image.shape[:2]                                                        # Store image dimensions\r\n",
        "  imageBlob = cv2.dnn.blobFromImage(                                              # Construct a blob from the image\r\n",
        "  \tcv2.resize(image, (300, 300)), 1.0, (300, 300),\r\n",
        "  \t(104.0, 177.0, 123.0), swapRB=False, crop=False)\r\n",
        "\r\n",
        "  detector.setInput(imageBlob)                                                    # Face Detector to localize face in an image\r\n",
        "  detections = detector.forward()       \r\n",
        "\r\n",
        "  if len(detections) > 0:                                                         # Ensure at least one face was found\r\n",
        "\t\ti = np.argmax(detections[0, 0, :, 2])                                         # Bounding box with largest probability\r\n",
        "\t\tconfidence = detections[0, 0, i, 2]        \r\n",
        "\r\n",
        "    if confidence > args[\"confidence\"]:                                           # Filtering weak detections\r\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])                     # Coordinates for bounding box of face\r\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\t\t\t\r\n",
        "\t\t\tface = image[startY:endY, startX:endX]                                      # Dimensions of ROI (Region of Interest)\r\n",
        "\t\t\t(fH, fW) = face.shape[:2]\r\n",
        "\t\t\t\r\n",
        "\t\t\tif (fW < 20 or fH < 20):                                                    # Face height and width need to be above a threshold\r\n",
        "\t\t\t\tcontinue    \r\n",
        "\r\n",
        "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), \r\n",
        "                                       swapRB=True, crop=False)                   # Create blob\r\n",
        "\r\n",
        "      embedder.setInput(faceBlob)\r\n",
        "      vec = embedder.forward()\r\n",
        "\r\n",
        "      knownNames.append(name)                                                     # Append name to list\r\n",
        "      knownEmbeddings.append(vec.flatten())                                       # Append flattened embedding to list\r\n",
        "      total += 1                                                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glLTq5YeYOLZ"
      },
      "source": [
        "# Save (pickle) the embeddings and the names\r\n",
        "\r\n",
        "print ('Serializing {} encodings:\\n'.format(total))\r\n",
        "\r\n",
        "data = {'embeddings': knownEmbeddings, 'names': knownNames}                       # Saved as a dictionary/ HashMap\r\n",
        "f = open('/content/drive/MyDrive/Open Lab/Output Embeddings', 'wb')\r\n",
        "f.write(pickle.dumps(data))                                                       # Stored as a ByteStream\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}